<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enhanced Nystagmus Eye-Tracking System</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-tomorrow.min.css">
    <style>
        .code-block {
            max-height: none;
            overflow: visible;
        }
        pre {
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        .chart-container {
            position: relative;
            height: 300px;
            width: 100%;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-900">
    <div class="container mx-auto px-4 py-8 max-w-6xl">
        
        <!-- Header -->
        <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
            <div class="text-center">
                <h1 class="text-4xl font-bold text-blue-600 mb-4">
                    <i class="fas fa-eye mr-3"></i>Enhanced Nystagmus Eye-Tracking System
                </h1>
                <p class="text-lg text-gray-600">Comprehensive guide for improved nystagmus detection using MediaPipe Face Mesh with 3D tracking, enhanced ML features, and real-time visualization</p>
            </div>
        </div>

        <!-- Key Improvements Overview -->
        <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
            <h2 class="text-3xl font-semibold text-gray-800 mb-6">
                <i class="fas fa-rocket mr-3"></i>Key Enhancements
            </h2>
            
            <div class="grid md:grid-cols-2 gap-6">
                <div class="bg-blue-50 p-6 rounded-lg">
                    <h3 class="text-xl font-semibold text-blue-800 mb-3">
                        <i class="fas fa-toggle-on mr-2"></i>Input Source Toggle
                    </h3>
                    <p class="text-gray-700">Code-based toggle system to switch between webcam and video file input without GUI dependencies. Simple comment/uncomment system for easy switching.</p>
                </div>
                
                <div class="bg-green-50 p-6 rounded-lg">
                    <h3 class="text-xl font-semibold text-green-800 mb-3">
                        <i class="fas fa-cube mr-2"></i>3D Landmark Integration
                    </h3>
                    <p class="text-gray-700">Enhanced tracking accuracy using MediaPipe's 3D landmarks (x, y, z coordinates) for improved depth perception and movement detection.</p>
                </div>
                
                <div class="bg-purple-50 p-6 rounded-lg">
                    <h3 class="text-xl font-semibold text-purple-800 mb-3">
                        <i class="fas fa-arrows-alt mr-2"></i>Independent Axis Tracking
                    </h3>
                    <p class="text-gray-700">Separate tracking of X and Y axis movements instead of radial distance, providing more precise nystagmus pattern analysis.</p>
                </div>
                
                <div class="bg-orange-50 p-6 rounded-lg">
                    <h3 class="text-xl font-semibold text-orange-800 mb-3">
                        <i class="fas fa-brain mr-2"></i>Enhanced ML Features
                    </h3>
                    <p class="text-gray-700">Advanced feature engineering including movement variance, blink detection, acceleration analysis, and smoothed velocity calculations.</p>
                </div>
                
                <div class="bg-red-50 p-6 rounded-lg">
                    <h3 class="text-xl font-semibold text-red-800 mb-3">
                        <i class="fas fa-chart-line mr-2"></i>Real-time Visualization
                    </h3>
                    <p class="text-gray-700">Live graphical display of eye movement features including velocity, amplitude, and frequency patterns for both eyes.</p>
                </div>
                
                <div class="bg-yellow-50 p-6 rounded-lg">
                    <h3 class="text-xl font-semibold text-yellow-800 mb-3">
                        <i class="fas fa-shield-alt mr-2"></i>Robust Error Handling
                    </h3>
                    <p class="text-gray-700">Improved handling of missing face frames, blink detection, and detection failures with graceful degradation and recovery mechanisms.</p>
                </div>
            </div>
        </div>

        <!-- Enhanced Code Structure -->
        <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
            <h2 class="text-3xl font-semibold text-gray-800 mb-6">
                <i class="fas fa-code mr-3"></i>Enhanced Code Implementation
            </h2>
            
            <div class="mb-6">
                <div class="bg-blue-100 border-l-4 border-blue-500 p-4 mb-4">
                    <p class="text-blue-800"><strong>Note:</strong> This enhanced version maintains your core structure while adding significant improvements for accuracy and functionality.</p>
                </div>
            </div>

            <pre class="code-block"><code class="language-python">#!/usr/bin/env python3
"""
Enhanced Nystagmus Eye-Tracking System
=====================================
Improved version with 3D tracking, enhanced ML features, and real-time visualization
"""

import cv2
import numpy as np
import datetime
import mediapipe as mp
import pandas as pd
import os
import time
from collections import deque
from openpyxl import Workbook, load_workbook
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from joblib import dump, load
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import threading
import queue

# Suppress TensorFlow warnings
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# =============================================================================
# CONFIGURATION SECTION - Easy Toggle System
# =============================================================================

# INPUT SOURCE CONFIGURATION (Toggle here)
USE_WEBCAM = True  # Set to True for webcam, False for video file
VIDEO_FILE_PATH = "path/to/your/video.mp4"  # Update this path when using video file

# TRACKING CONFIGURATION
USE_3D_LANDMARKS = True  # Enable 3D tracking (x, y, z)
ENABLE_REAL_TIME_VISUALIZATION = True  # Enable live graphs
BUFFER_SIZE = 100  # Number of frames to keep in memory for analysis
SMOOTHING_WINDOW = 5  # Window size for smoothing calculations

# =============================================================================
# ENHANCED EYE TRACKING CLASS
# =============================================================================

class EnhancedEyeTracker:
    def __init__(self):
        # Initialize MediaPipe Face Mesh with refined landmarks
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.3,
            min_tracking_confidence=0.3,
        )
        
        # Eye landmark indices (MediaPipe Face Mesh)
        self.left_eye_landmarks = {
            'center': 468,
            'left': 471,
            'right': 469,
            'top': 470,
            'bottom': 472
        }
        
        self.right_eye_landmarks = {
            'center': 473,
            'left': 476,
            'right': 474,
            'top': 475,
            'bottom': 477
        }
        
        # Nose reference point
        self.nose_tip = 6
        
        # Data buffers for temporal analysis
        self.left_eye_buffer = deque(maxlen=BUFFER_SIZE)
        self.right_eye_buffer = deque(maxlen=BUFFER_SIZE)
        self.time_buffer = deque(maxlen=BUFFER_SIZE)
        
        # Enhanced tracking variables
        self.previous_left_pos = None
        self.previous_right_pos = None
        self.blink_counter_left = 0
        self.blink_counter_right = 0
        self.last_blink_time_left = 0
        self.last_blink_time_right = 0
        
        # Feature calculation variables
        self.oscillation_count_left_x = 0
        self.oscillation_count_left_y = 0
        self.oscillation_count_right_x = 0
        self.oscillation_count_right_y = 0
        
        # Model and scaler
        self.model = None
        self.scaler = None
        self.load_or_create_model()
        
        # Visualization setup
        if ENABLE_REAL_TIME_VISUALIZATION:
            self.setup_visualization()
    
    def setup_visualization(self):
        """Setup real-time visualization"""
        self.fig, self.axes = plt.subplots(2, 2, figsize=(12, 8))
        self.fig.suptitle('Real-time Eye Movement Analysis')
        
        # Initialize plot data
        self.plot_data = {
            'time': deque(maxlen=BUFFER_SIZE),
            'left_vel_x': deque(maxlen=BUFFER_SIZE),
            'left_vel_y': deque(maxlen=BUFFER_SIZE),
            'right_vel_x': deque(maxlen=BUFFER_SIZE),
            'right_vel_y': deque(maxlen=BUFFER_SIZE),
            'left_amp': deque(maxlen=BUFFER_SIZE),
            'right_amp': deque(maxlen=BUFFER_SIZE),
            'left_freq': deque(maxlen=BUFFER_SIZE),
            'right_freq': deque(maxlen=BUFFER_SIZE)
        }
        
        plt.ion()  # Interactive mode
    
    def extract_3d_landmarks(self, face_landmarks, landmark_indices, frame_shape):
        """Extract 3D coordinates for specified landmarks"""
        h, w = frame_shape[:2]
        landmarks = {}
        
        for name, idx in landmark_indices.items():
            landmark = face_landmarks.landmark[idx]
            if USE_3D_LANDMARKS:
                landmarks[name] = np.array([
                    landmark.x * w,
                    landmark.y * h,
                    landmark.z * w  # Relative depth
                ])
            else:
                landmarks[name] = np.array([
                    landmark.x * w,
                    landmark.y * h
                ])
        
        return landmarks
    
    def calculate_eye_openness(self, eye_landmarks):
        """Calculate eye openness ratio for blink detection"""
        if USE_3D_LANDMARKS:
            vertical_dist = np.linalg.norm(eye_landmarks['top'] - eye_landmarks['bottom'])
            horizontal_dist = np.linalg.norm(eye_landmarks['left'] - eye_landmarks['right'])
        else:
            vertical_dist = np.linalg.norm(eye_landmarks['top'][:2] - eye_landmarks['bottom'][:2])
            horizontal_dist = np.linalg.norm(eye_landmarks['left'][:2] - eye_landmarks['right'][:2])
        
        # Avoid division by zero
        if horizontal_dist < 1e-6:
            return 0
        
        return vertical_dist / horizontal_dist
    
    def detect_blink(self, eye_openness, threshold=0.2):
        """Detect blink based on eye openness ratio"""
        return eye_openness < threshold
    
    def calculate_movement_features(self, current_pos, previous_pos, dt):
        """Calculate enhanced movement features"""
        if previous_pos is None or dt <= 0:
            return {
                'displacement_x': 0, 'displacement_y': 0,
                'velocity_x': 0, 'velocity_y': 0,
                'acceleration_x': 0, 'acceleration_y': 0,
                'amplitude': 0, 'total_velocity': 0
            }
        
        # Calculate displacement
        displacement = current_pos - previous_pos
        displacement_x = displacement[0] if len(displacement) > 0 else 0
        displacement_y = displacement[1] if len(displacement) > 1 else 0
        
        # Calculate velocity
        velocity_x = displacement_x / dt
        velocity_y = displacement_y / dt
        total_velocity = np.sqrt(velocity_x**2 + velocity_y**2)
        
        # Calculate amplitude (magnitude of displacement)
        amplitude = np.sqrt(displacement_x**2 + displacement_y**2)
        
        return {
            'displacement_x': displacement_x,
            'displacement_y': displacement_y,
            'velocity_x': velocity_x,
            'velocity_y': velocity_y,
            'amplitude': amplitude,
            'total_velocity': total_velocity
        }
    
    def calculate_frequency(self, buffer, axis_index, time_buffer):
        """Calculate frequency using zero-crossing detection"""
        if len(buffer) < 3 or len(time_buffer) < 3:
            return 0
        
        # Extract position data for specified axis
        positions = [pos[axis_index] if len(pos) > axis_index else 0 for pos in buffer]
        
        # Find zero crossings (sign changes in velocity)
        velocities = np.diff(positions)
        zero_crossings = 0
        
        for i in range(1, len(velocities)):
            if velocities[i] * velocities[i-1] < 0:
                zero_crossings += 1
        
        # Calculate frequency (half the zero crossings per unit time)
        if len(time_buffer) > 1:
            total_time = time_buffer[-1] - time_buffer[0]
            if total_time > 0:
                return (zero_crossings / 2) / total_time
        
        return 0
    
    def calculate_variance(self, buffer, axis_index):
        """Calculate movement variance for specified axis"""
        if len(buffer) < 2:
            return 0
        
        positions = [pos[axis_index] if len(pos) > axis_index else 0 for pos in buffer]
        return np.var(positions)
    
    def smooth_data(self, data, window_size=SMOOTHING_WINDOW):
        """Apply moving average smoothing"""
        if len(data) < window_size:
            return data[-1] if data else 0
        
        return np.mean(data[-window_size:])
    
    def load_or_create_model(self):
        """Load existing model or create new one"""
        MODEL_PATH = 'enhanced_tremor_model.joblib'
        SCALER_PATH = 'enhanced_tremor_scaler.joblib'
        DATA_PATH = r"F:\GP\ML\LiveData\LiveData.xlsx"
        
        try:
            if os.path.exists(MODEL_PATH) and os.path.exists(SCALER_PATH):
                self.model = load(MODEL_PATH)
                self.scaler = load(SCALER_PATH)
                print("‚úì Loaded pre-trained enhanced model successfully!")
            else:
                raise FileNotFoundError
        except Exception as e:
            print(f"No enhanced model found: {e}. Training new model...")
            self.train_new_model(DATA_PATH, MODEL_PATH, SCALER_PATH)
    
    def train_new_model(self, data_path, model_path, scaler_path):
        """Train new model with enhanced features"""
        if not os.path.exists(data_path):
            print("No training data available. Predictions disabled.")
            return
        
        try:
            df = pd.read_excel(data_path)
            
            # Enhanced feature set
            enhanced_features = [
                'LeftMove_X', 'LeftMove_Y', 'RightMove_X', 'RightMove_Y',
                'LeftVel_X', 'LeftVel_Y', 'RightVel_X', 'RightVel_Y',
                'LeftFreq_X', 'LeftFreq_Y', 'RightFreq_X', 'RightFreq_Y',
                'LeftAmp', 'RightAmp', 'LeftVar_X', 'LeftVar_Y',
                'RightVar_X', 'RightVar_Y', 'BlinkRate_Left', 'BlinkRate_Right'
            ]
            
            # Add missing columns with default values
            for feature in enhanced_features:
                if feature not in df.columns:
                    df[feature] = 0
            
            if 'Label' not in df.columns:
                df['Label'] = 0
            
            df = df.dropna()
            X = df[enhanced_features]
            y = df['Label']
            
            if len(X) > 0:
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=0.2, random_state=42
                )
                
                self.scaler = StandardScaler()
                X_train_scaled = self.scaler.fit_transform(X_train)
                X_test_scaled = self.scaler.transform(X_test)
                
                self.model = RandomForestClassifier(
                    n_estimators=150,
                    max_depth=8,
                    random_state=42,
                    class_weight='balanced'
                )
                self.model.fit(X_train_scaled, y_train)
                
                dump(self.model, model_path)
                dump(self.scaler, scaler_path)
                
                accuracy = self.model.score(X_test_scaled, y_test)
                print(f"‚úì Enhanced model trained with accuracy: {accuracy:.3f}")
        
        except Exception as e:
            print(f"Error training model: {e}")
    
    def process_frame(self, frame, timestamp):
        """Process single frame and return enhanced features"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_frame)
        
        frame_data = {
            'face_detected': False,
            'left_eye': {},
            'right_eye': {},
            'prediction': "No face detected",
            'blink_left': False,
            'blink_right': False
        }
        
        if not results.multi_face_landmarks:
            return frame_data
        
        for face_landmarks in results.multi_face_landmarks:
            h, w = frame.shape[:2]
            frame_data['face_detected'] = True
            
            # Extract eye landmarks
            left_landmarks = self.extract_3d_landmarks(
                face_landmarks, self.left_eye_landmarks, frame.shape
            )
            right_landmarks = self.extract_3d_landmarks(
                face_landmarks, self.right_eye_landmarks, frame.shape
            )
            
            # Get current positions (center of eyes)
            current_left_pos = left_landmarks['center']
            current_right_pos = right_landmarks['center']
            
            # Calculate time difference
            dt = 1/30  # Default fps, should be calculated from actual fps
            
            # Calculate movement features
            left_features = self.calculate_movement_features(
                current_left_pos, self.previous_left_pos, dt
            )
            right_features = self.calculate_movement_features(
                current_right_pos, self.previous_right_pos, dt
            )
            
            # Blink detection
            left_openness = self.calculate_eye_openness(left_landmarks)
            right_openness = self.calculate_eye_openness(right_landmarks)
            
            blink_left = self.detect_blink(left_openness)
            blink_right = self.detect_blink(right_openness)
            
            if blink_left:
                self.blink_counter_left += 1
                self.last_blink_time_left = timestamp
            
            if blink_right:
                self.blink_counter_right += 1
                self.last_blink_time_right = timestamp
            
            # Update buffers
            self.left_eye_buffer.append(current_left_pos)
            self.right_eye_buffer.append(current_right_pos)
            self.time_buffer.append(timestamp)
            
            # Calculate frequencies and variances
            left_freq_x = self.calculate_frequency(self.left_eye_buffer, 0, self.time_buffer)
            left_freq_y = self.calculate_frequency(self.left_eye_buffer, 1, self.time_buffer)
            right_freq_x = self.calculate_frequency(self.right_eye_buffer, 0, self.time_buffer)
            right_freq_y = self.calculate_frequency(self.right_eye_buffer, 1, self.time_buffer)
            
            left_var_x = self.calculate_variance(self.left_eye_buffer, 0)
            left_var_y = self.calculate_variance(self.left_eye_buffer, 1)
            right_var_x = self.calculate_variance(self.right_eye_buffer, 0)
            right_var_y = self.calculate_variance(self.right_eye_buffer, 1)
            
            # Calculate blink rates
            time_window = 10  # seconds
            current_time = timestamp
            blink_rate_left = self.blink_counter_left / max(current_time, 1)
            blink_rate_right = self.blink_counter_right / max(current_time, 1)
            
            # Prepare enhanced features for ML model
            enhanced_features = [
                left_features['displacement_x'], left_features['displacement_y'],
                right_features['displacement_x'], right_features['displacement_y'],
                left_features['velocity_x'], left_features['velocity_y'],
                right_features['velocity_x'], right_features['velocity_y'],
                left_freq_x, left_freq_y, right_freq_x, right_freq_y,
                left_features['amplitude'], right_features['amplitude'],
                left_var_x, left_var_y, right_var_x, right_var_y,
                blink_rate_left, blink_rate_right
            ]
            
            # Make prediction
            prediction = "Normal"
            confidence = 0.0
            
            if self.model and self.scaler:
                try:
                    if len(enhanced_features) == 20:  # Expected feature count
                        scaled_features = self.scaler.transform([enhanced_features])
                        pred = self.model.predict(scaled_features)[0]
                        proba = self.model.predict_proba(scaled_features)[0]
                        confidence = np.max(proba)
                        prediction = f"State: {pred} ({confidence:.3f})"
                except Exception as e:
                    prediction = f"Prediction error: {str(e)[:20]}"
            
            # Store frame data
            frame_data.update({
                'left_eye': {
                    'position': current_left_pos,
                    'displacement_x': left_features['displacement_x'],
                    'displacement_y': left_features['displacement_y'],
                    'velocity_x': left_features['velocity_x'],
                    'velocity_y': left_features['velocity_y'],
                    'amplitude': left_features['amplitude'],
                    'frequency_x': left_freq_x,
                    'frequency_y': left_freq_y,
                    'variance_x': left_var_x,
                    'variance_y': left_var_y,
                    'openness': left_openness
                },
                'right_eye': {
                    'position': current_right_pos,
                    'displacement_x': right_features['displacement_x'],
                    'displacement_y': right_features['displacement_y'],
                    'velocity_x': right_features['velocity_x'],
                    'velocity_y': right_features['velocity_y'],
                    'amplitude': right_features['amplitude'],
                    'frequency_x': right_freq_x,
                    'frequency_y': right_freq_y,
                    'variance_x': right_var_x,
                    'variance_y': right_var_y,
                    'openness': right_openness
                },
                'prediction': prediction,
                'blink_left': blink_left,
                'blink_right': blink_right,
                'blink_rate_left': blink_rate_left,
                'blink_rate_right': blink_rate_right
            })
            
            # Update previous positions
            self.previous_left_pos = current_left_pos.copy()
            self.previous_right_pos = current_right_pos.copy()
            
            break  # Process only first face
        
        return frame_data
    
    def update_visualization(self, frame_data, timestamp):
        """Update real-time visualization"""
        if not ENABLE_REAL_TIME_VISUALIZATION or not frame_data['face_detected']:
            return
        
        # Update plot data
        self.plot_data['time'].append(timestamp)
        
        left_eye = frame_data['left_eye']
        right_eye = frame_data['right_eye']
        
        self.plot_data['left_vel_x'].append(left_eye['velocity_x'])
        self.plot_data['left_vel_y'].append(left_eye['velocity_y'])
        self.plot_data['right_vel_x'].append(right_eye['velocity_x'])
        self.plot_data['right_vel_y'].append(right_eye['velocity_y'])
        self.plot_data['left_amp'].append(left_eye['amplitude'])
        self.plot_data['right_amp'].append(right_eye['amplitude'])
        self.plot_data['left_freq'].append(left_eye['frequency_x'])
        self.plot_data['right_freq'].append(right_eye['frequency_x'])
        
        # Update plots every 10 frames to reduce overhead
        if len(self.plot_data['time']) % 10 == 0:
            self.refresh_plots()
    
    def refresh_plots(self):
        """Refresh visualization plots"""
        if not ENABLE_REAL_TIME_VISUALIZATION:
            return
        
        # Clear previous plots
        for ax in self.axes.flat:
            ax.clear()
        
        time_data = list(self.plot_data['time'])
        
        if len(time_data) < 2:
            return
        
        # Velocity plot
        self.axes[0, 0].plot(time_data, list(self.plot_data['left_vel_x']), 'b-', label='Left X')
        self.axes[0, 0].plot(time_data, list(self.plot_data['left_vel_y']), 'b--', label='Left Y')
        self.axes[0, 0].plot(time_data, list(self.plot_data['right_vel_x']), 'r-', label='Right X')
        self.axes[0, 0].plot(time_data, list(self.plot_data['right_vel_y']), 'r--', label='Right Y')
        self.axes[0, 0].set_title('Eye Velocity (pixels/s)')
        self.axes[0, 0].legend()
        self.axes[0, 0].grid(True)
        
        # Amplitude plot
        self.axes[0, 1].plot(time_data, list(self.plot_data['left_amp']), 'b-', label='Left Eye')
        self.axes[0, 1].plot(time_data, list(self.plot_data['right_amp']), 'r-', label='Right Eye')
        self.axes[0, 1].set_title('Movement Amplitude')
        self.axes[0, 1].legend()
        self.axes[0, 1].grid(True)
        
        # Frequency plot
        self.axes[1, 0].plot(time_data, list(self.plot_data['left_freq']), 'b-', label='Left Eye')
        self.axes[1, 0].plot(time_data, list(self.plot_data['right_freq']), 'r-', label='Right Eye')
        self.axes[1, 0].set_title('Movement Frequency (Hz)')
        self.axes[1, 0].legend()
        self.axes[1, 0].grid(True)
        
        # Combined analysis
        if len(time_data) > 1:
            combined_movement = np.array(list(self.plot_data['left_amp'])) + np.array(list(self.plot_data['right_amp']))
            self.axes[1, 1].plot(time_data, combined_movement, 'g-', label='Combined Movement')
            self.axes[1, 1].set_title('Combined Eye Movement')
            self.axes[1, 1].legend()
            self.axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.pause(0.001)

# =============================================================================
# MAIN APPLICATION
# =============================================================================

def main():
    """Main application loop"""
    print("üöÄ Starting Enhanced Nystagmus Eye-Tracking System")
    print("=" * 60)
    
    # Initialize video capture based on configuration
    if USE_WEBCAM:
        print("üìπ Initializing webcam...")
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("‚ùå Error: Could not open webcam.")
            return
        print("‚úì Webcam initialized successfully")
    else:
        print(f"üìÅ Loading video file: {VIDEO_FILE_PATH}")
        cap = cv2.VideoCapture(VIDEO_FILE_PATH)
        if not cap.isOpened():
            print("‚ùå Error: Could not open video file.")
            return
        print("‚úì Video file loaded successfully")
    
    # Get video properties
    fps = cap.get(cv2.CAP_PROP_FPS) or 30
    frame_time = 1 / fps
    print(f"üìä Video FPS: {fps:.2f}")
    
    # Initialize enhanced tracker
    print("üîß Initializing enhanced eye tracker...")
    tracker = EnhancedEyeTracker()
    print("‚úì Eye tracker initialized")
    
    # Data collection setup
    data_collection = []
    frame_index = 0
    start_time = time.time()
    
    print("\nüéØ Starting tracking session...")
    print("Press 'ESC' to stop, 'S' to save current session")
    print("=" * 60)
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("üì∫ End of video/stream reached")
                break
            
            # Flip frame if using webcam
            if USE_WEBCAM:
                frame = cv2.flip(frame, 1)
            
            current_time = time.time() - start_time
            
            # Process frame
            frame_data = tracker.process_frame(frame, current_time)
            
            # Update visualization
            tracker.update_visualization(frame_data, current_time)
            
            # Collect data
            if frame_data['face_detected']:
                left_eye = frame_data['left_eye']
                right_eye = frame_data['right_eye']
                
                row_data = [
                    frame_index, current_time,
                    left_eye['displacement_x'], left_eye['displacement_y'],
                    right_eye['displacement_x'], right_eye['displacement_y'],
                    left_eye['velocity_x'], left_eye['velocity_y'],
                    right_eye['velocity_x'], right_eye['velocity_y'],
                    left_eye['frequency_x'], left_eye['frequency_y'],
                    right_eye['frequency_x'], right_eye['frequency_y'],
                    left_eye['amplitude'], right_eye['amplitude'],
                    left_eye['variance_x'], left_eye['variance_y'],
                    right_eye['variance_x'], right_eye['variance_y'],
                    frame_data['blink_rate_left'], frame_data['blink_rate_right'],
                    frame_data['prediction']
                ]
                data_collection.append(row_data)
            
            # Display frame with information
            display_frame = frame.copy()
            
            # Add status information
            status_color = (0, 255, 0) if frame_data['face_detected'] else (0, 0, 255)
            cv2.putText(display_frame, frame_data['prediction'], (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)
            
            if frame_data['face_detected']:
                # Show blink status
                blink_status = f"Blinks: L:{frame_data['blink_rate_left']:.2f} R:{frame_data['blink_rate_right']:.2f}"
                cv2.putText(display_frame, blink_status, (10, 60),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                # Show movement info
                left_eye = frame_data['left_eye']
                right_eye = frame_data['right_eye']
                movement_info = f"Vel: L({left_eye['velocity_x']:.1f},{left_eye['velocity_y']:.1f}) R({right_eye['velocity_x']:.1f},{right_eye['velocity_y']:.1f})"
                cv2.putText(display_frame, movement_info, (10, 90),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # Show frame count and time
            info_text = f"Frame: {frame_index} | Time: {current_time:.2f}s"
            cv2.putText(display_frame, info_text, (10, display_frame.shape[0] - 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            cv2.imshow("Enhanced Nystagmus Detection", display_frame)
            
            # Handle key presses
            key = cv2.waitKey(1) & 0xFF
            if key == 27:  # ESC key
                break
            elif key == ord('s') or key == ord('S'):  # Save data
                save_session_data(data_collection)
            
            frame_index += 1
    
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Session interrupted by user")
    
    finally:
        # Cleanup
        print("\nüîÑ Cleaning up resources...")
        save_session_data(data_collection)
        cap.release()
        cv2.destroyAllWindows()
        if ENABLE_REAL_TIME_VISUALIZATION:
            plt.close('all')
        print("‚úì Cleanup completed")

def save_session_data(data_collection):
    """Save collected data to Excel file"""
    if not data_collection:
        print("‚ö†Ô∏è  No data to save")
        return
    
    output_dir = r"F:\GP\ML\LiveData"
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, "EnhancedLiveData.xlsx")
    
    try:
        # Create DataFrame
        columns = [
            'Index', 'Time', 'LeftMove_X', 'LeftMove_Y', 'RightMove_X', 'RightMove_Y',
            'LeftVel_X', 'LeftVel_Y', 'RightVel_X', 'RightVel_Y',
            'LeftFreq_X', 'LeftFreq_Y', 'RightFreq_X', 'RightFreq_Y',
            'LeftAmp', 'RightAmp', 'LeftVar_X', 'LeftVar_Y', 'RightVar_X', 'RightVar_Y',
            'BlinkRate_Left', 'BlinkRate_Right', 'Prediction'
        ]
        
        df = pd.DataFrame(data_collection, columns=columns)
        
        # Create workbook
        if not os.path.exists(output_file):
            wb = Workbook()
            wb.save(output_file)
        
        wb = load_workbook(output_file)
        sheet_name = f"Enhanced_Session_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Write data
        with pd.ExcelWriter(output_file, engine='openpyxl', mode='a') as writer:
            df.to_excel(writer, sheet_name=sheet_name, index=False)
        
        print(f"‚úÖ Data saved successfully to {sheet_name}")
        print(f"üìä Saved {len(data_collection)} frames of data")
        
    except Exception as e:
        print(f"‚ùå Error saving data: {str(e)}")

if __name__ == "__main__":
    main()
</code></pre>
        </div>

        <!-- Feature Engineering Details -->
        <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
            <h2 class="text-3xl font-semibold text-gray-800 mb-6">
                <i class="fas fa-cogs mr-3"></i>Enhanced Feature Engineering
            </h2>
            
            <div class="grid md:grid-cols-2 gap-8">
                <div>
                    <h3 class="text-xl font-semibold text-blue-800 mb-4">Movement Analysis Features</h3>
                    <div class="space-y-4">
                        <div class="bg-gray-50 p-4 rounded-lg">
                            <h4 class="font-semibold text-gray-800">Independent X/Y Tracking</h4>
                            <p class="text-sm text-gray-600">Separate velocity, displacement, and frequency calculations for horizontal and vertical movements, providing more precise nystagmus pattern detection.</p>
                        </div>
                        <div class="bg-gray-50 p-4 rounded-lg">
                            <h4 class="font-semibold text-gray-800">Movement Variance</h4>
                            <p class="text-sm text-gray-600">Statistical variance analysis of eye positions over time windows to detect irregular movement patterns characteristic of nystagmus.</p>
                        </div>
                        <div class="bg-gray-50 p-4 rounded-lg">
                            <h4 class="font-semibold text-gray-800">Velocity Derivatives</h4>
                            <p class="text-sm text-gray-600">Acceleration calculations from velocity changes to identify sudden movement changes and jerky patterns.</p>
                        </div>
                    </div>
                </div>
                
                <div>
                    <h3 class="text-xl font-semibold text-green-800 mb-4">Behavioral Features</h3>
                    <div class="space-y-4">
                        <div class="bg-gray-50 p-4 rounded-lg">
                            <h4 class="font-semibold text-gray-800">Blink Pattern Detection</h4>
                            <p class="text-sm text-gray-600">Eye openness ratio calculation and blink rate analysis to distinguish between voluntary blinks and involuntary eye movements.</p>
                        </div>
                        <div class="bg-gray-50 p-4 rounded-lg">
                            <h4 class="font-semibold text-gray-800">Temporal Smoothing</h4>
                            <p class="text-sm text-gray-600">Moving average smoothing applied to reduce noise and improve accuracy of movement detection algorithms.</p>
                        </div>
                        <div class="bg-gray-50 p-4 rounded-lg">
                            <h4 class="font-semibold text-gray-800">Frequency Analysis</h4>
                            <p class="text-sm text-gray-600">Zero-crossing detection for accurate frequency calculation of oscillatory movements in both axes independently.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Visualization Examples -->
        <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
            <h2 class="text-3xl font-semibold text-gray-800 mb-6">
                <i class="fas fa-chart-area mr-3"></i>Real-time Visualization Examples
            </h2>
            
            <div class="grid md:grid-cols-2 gap-6 mb-6">
                <div class="chart-container">
                    <canvas id="velocityChart"></canvas>
                </div>
                <div class="chart-container">
                    <canvas id="amplitudeChart"></canvas>
                </div>
            </div>
            
            <div class="grid md:grid-cols-2 gap-6">
                <div class="chart-container">
                    <canvas id="frequencyChart"></canvas>
                </div>
                <div class="chart-container">
                    <canvas id="combinedChart"></canvas>
                </div>
            </div>
        </div>

        <!-- Usage Instructions -->
        <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
            <h2 class="text-3xl font-semibold text-gray-800 mb-6">
                <i class="fas fa-play-circle mr-3"></i>Usage Instructions
            </h2>
            
            <div class="space-y-6">
                <div class="bg-blue-50 border-l-4 border-blue-500 p-6">
                    <h3 class="text-lg font-semibold text-blue-800 mb-3">Input Source Configuration</h3>
                    <p class="text-blue-700 mb-2">To switch between webcam and video file input, modify these variables at the top of the code:</p>
                    <pre class="bg-blue-100 p-3 rounded text-sm"><code># For webcam
USE_WEBCAM = True

# For video file
USE_WEBCAM = False
VIDEO_FILE_PATH = "path/to/your/video.mp4"</code></pre>
                </div>
                
                <div class="bg-green-50 border-l-4 border-green-500 p-6">
                    <h3 class="text-lg font-semibold text-green-800 mb-3">3D Tracking Toggle</h3>
                    <p class="text-green-700 mb-2">Enable or disable 3D landmark tracking:</p>
                    <pre class="bg-green-100 p-3 rounded text-sm"><code>USE_3D_LANDMARKS = True  # For 3D tracking (x, y, z)
USE_3D_LANDMARKS = False  # For 2D tracking (x, y)</code></pre>
                </div>
                
                <div class="bg-purple-50 border-l-4 border-purple-500 p-6">
                    <h3 class="text-lg font-semibold text-purple-800 mb-3">Visualization Control</h3>
                    <p class="text-purple-700 mb-2">Control real-time visualization:</p>
                    <pre class="bg-purple-100 p-3 rounded text-sm"><code>ENABLE_REAL_TIME_VISUALIZATION = True  # Enable live graphs
BUFFER_SIZE = 100  # Number of frames to display
SMOOTHING_WINDOW = 5  # Smoothing parameter</code></pre>
                </div>
                
                <div class="bg-orange-50 border-l-4 border-orange-500 p-6">
                    <h3 class="text-lg font-semibold text-orange-800 mb-3">Runtime Controls</h3>
                    <ul class="text-orange-700 space-y-2">
                        <li><strong>ESC:</strong> Stop tracking session and save data</li>
                        <li><strong>S:</strong> Save current session data to Excel</li>
                        <li><strong>Real-time graphs:</strong> Automatically update during tracking</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Key Improvements Summary -->
        <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
            <h2 class="text-3xl font-semibold text-gray-800 mb-6">
                <i class="fas fa-star mr-3"></i>Why These Enhancements Matter
            </h2>
            
            <div class="grid md:grid-cols-3 gap-6">
                <div class="text-center">
                    <div class="bg-blue-100 rounded-full w-16 h-16 flex items-center justify-center mx-auto mb-4">
                        <i class="fas fa-bullseye text-2xl text-blue-600"></i>
                    </div>
                    <h3 class="text-lg font-semibold text-gray-800 mb-2">Improved Accuracy</h3>
                    <p class="text-gray-600 text-sm">3D tracking and independent axis analysis provide more precise nystagmus detection with reduced false positives.</p>
                </div>
                
                <div class="text-center">
                    <div class="bg-green-100 rounded-full w-16 h-16 flex items-center justify-center mx-auto mb-4">
                        <i class="fas fa-shield-alt text-2xl text-green-600"></i>
                    </div>
                    <h3 class="text-lg font-semibold text-gray-800 mb-2">Robust Performance</h3>
                    <p class="text-gray-600 text-sm">Enhanced error handling and blink detection ensure stable operation even with challenging input conditions.</p>
                </div>
                
                <div class="text-center">
                    <div class="bg-purple-100 rounded-full w-16 h-16 flex items-center justify-center mx-auto mb-4">
                        <i class="fas fa-brain text-2xl text-purple-600"></i>
                    </div>
                    <h3 class="text-lg font-semibold text-gray-800 mb-2">Advanced Analytics</h3>
                    <p class="text-gray-600 text-sm">Enhanced feature engineering with variance, acceleration, and pattern analysis for better ML model performance.</p>
                </div>
            </div>
        </div>

        <!-- Installation and Dependencies -->
        <div class="bg-white rounded-lg shadow-lg p-8">
            <h2 class="text-3xl font-semibold text-gray-800 mb-6">
                <i class="fas fa-download mr-3"></i>Installation and Dependencies
            </h2>
            
            <div class="bg-gray-50 p-6 rounded-lg">
                <h3 class="text-lg font-semibold text-gray-800 mb-4">Required Python Packages</h3>
                <pre class="bg-gray-800 text-green-400 p-4 rounded text-sm overflow-x-auto"><code>pip install opencv-python
pip install mediapipe
pip install pandas
pip install numpy
pip install scikit-learn
pip install matplotlib
pip install openpyxl
pip install joblib</code></pre>
                
                <div class="mt-6">
                    <h4 class="font-semibold text-gray-800 mb-2">System Requirements</h4>
                    <ul class="text-gray-600 space-y-1 text-sm">
                        <li>‚Ä¢ Python 3.7 or higher</li>
                        <li>‚Ä¢ Webcam for real-time tracking (optional)</li>
                        <li>‚Ä¢ Sufficient RAM for real-time visualization (8GB+ recommended)</li>
                        <li>‚Ä¢ GPU acceleration recommended for better performance</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <script>
        // Sample data for demonstration charts
        const timeLabels = Array.from({length: 50}, (_, i) => i * 0.1);
        const leftVelX = timeLabels.map(t => Math.sin(t * 2) * 10 + Math.random() * 5);
        const leftVelY = timeLabels.map(t => Math.cos(t * 1.5) * 8 + Math.random() * 3);
        const rightVelX = timeLabels.map(t => Math.sin(t * 2.2) * 12 + Math.random() * 4);
        const rightVelY = timeLabels.map(t => Math.cos(t * 1.8) * 9 + Math.random() * 3);

        // Velocity Chart
        const velocityCtx = document.getElementById('velocityChart').getContext('2d');
        new Chart(velocityCtx, {
            type: 'line',
            data: {
                labels: timeLabels,
                datasets: [{
                    label: 'Left Eye X Velocity',
                    data: leftVelX,
                    borderColor: 'rgb(59, 130, 246)',
                    backgroundColor: 'rgba(59, 130, 246, 0.1)',
                    tension: 0.4
                }, {
                    label: 'Left Eye Y Velocity',
                    data: leftVelY,
                    borderColor: 'rgb(139, 69, 19)',
                    backgroundColor: 'rgba(139, 69, 19, 0.1)',
                    tension: 0.4
                }, {
                    label: 'Right Eye X Velocity',
                    data: rightVelX,
                    borderColor: 'rgb(220, 38, 127)',
                    backgroundColor: 'rgba(220, 38, 127, 0.1)',
                    tension: 0.4
                }, {
                    label: 'Right Eye Y Velocity',
                    data: rightVelY,
                    borderColor: 'rgb(34, 197, 94)',
                    backgroundColor: 'rgba(34, 197, 94, 0.1)',
                    tension: 0.4
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    title: {
                        display: true,
                        text: 'Eye Velocity Tracking (pixels/s)'
                    }
                },
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'Time (s)'
                        }
                    },
                    y: {
                        title: {
                            display: true,
                            text: 'Velocity (pixels/s)'
                        }
                    }
                }
            }
        });

        // Amplitude Chart
        const amplitudeCtx = document.getElementById('amplitudeChart').getContext('2d');
        const leftAmp = timeLabels.map(t => Math.abs(Math.sin(t * 3)) * 15 + Math.random() * 3);
        const rightAmp = timeLabels.map(t => Math.abs(Math.cos(t * 2.5)) * 12 + Math.random() * 2);

        new Chart(amplitudeCtx, {
            type: 'line',
            data: {
                labels: timeLabels,
                datasets: [{
                    label: 'Left Eye Amplitude',
                    data: leftAmp,
                    borderColor: 'rgb(59, 130, 246)',
                    backgroundColor: 'rgba(59, 130, 246, 0.1)',
                    fill: true,
                    tension: 0.4
                }, {
                    label: 'Right Eye Amplitude',
                    data: rightAmp,
                    borderColor: 'rgb(220, 38, 127)',
                    backgroundColor: 'rgba(220, 38, 127, 0.1)',
                    fill: true,
                    tension: 0.4
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    title: {
                        display: true,
                        text: 'Movement Amplitude'
                    }
                },
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'Time (s)'
                        }
                    },
                    y: {
                        title: {
                            display: true,
                            text: 'Amplitude (pixels)'
                        }
                    }
                }
            }
        });

        // Frequency Chart
        const frequencyCtx = document.getElementById('frequencyChart').getContext('2d');
        const leftFreq = timeLabels.map(t => 2 + Math.sin(t * 0.5) * 0.5 + Math.random() * 0.3);
        const rightFreq = timeLabels.map(t => 2.2 + Math.cos(t * 0.7) * 0.4 + Math.random() * 0.2);

        new Chart(frequencyCtx, {
            type: 'line',
            data: {
                labels: timeLabels,
                datasets: [{
                    label: 'Left Eye Frequency',
                    data: leftFreq,
                    borderColor: 'rgb(34, 197, 94)',
                    backgroundColor: 'rgba(34, 197, 94, 0.1)',
                    tension: 0.4
                }, {
                    label: 'Right Eye Frequency',
                    data: rightFreq,
                    borderColor: 'rgb(239, 68, 68)',
                    backgroundColor: 'rgba(239, 68, 68, 0.1)',
                    tension: 0.4
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    title: {
                        display: true,
                        text: 'Movement Frequency (Hz)'
                    }
                },
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'Time (s)'
                        }
                    },
                    y: {
                        title: {
                            display: true,
                            text: 'Frequency (Hz)'
                        }
                    }
                }
            }
        });

        // Combined Movement Chart
        const combinedCtx = document.getElementById('combinedChart').getContext('2d');
        const combinedMovement = leftAmp.map((left, i) => left + rightAmp[i]);

        new Chart(combinedCtx, {
            type: 'line',
            data: {
                labels: timeLabels,
                datasets: [{
                    label: 'Combined Eye Movement',
                    data: combinedMovement,
                    borderColor: 'rgb(147, 51, 234)',
                    backgroundColor: 'rgba(147, 51, 234, 0.1)',
                    fill: true,
                    tension: 0.4
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    title: {
                        display: true,
                        text: 'Combined Eye Movement Analysis'
                    }
                },
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'Time (s)'
                        }
                    },
                    y: {
                        title: {
                            display: true,
                            text: 'Combined Movement (pixels)'
                        }
                    }
                }
            }
        });
    </script>
</body>
</html>